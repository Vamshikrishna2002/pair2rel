{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b96221d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bt19d200/NER_Vamshi/NER_Model/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0969bee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"DFKI-SLT/nyt-multi\") \n",
    "train = dataset[\"train\"]\n",
    "test = dataset[\"test\"]\n",
    "rel_set=set()\n",
    "for i in range(len(test)):\n",
    "    s=test[i]['relations'][0]['type']\n",
    "    result = s.split('/')[3]  # Index 3 gives you the part after the second slash\n",
    "    rel_set.add(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10165176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'administrative_divisions',\n",
       " 'advisors',\n",
       " 'capital',\n",
       " 'children',\n",
       " 'company',\n",
       " 'contains',\n",
       " 'country',\n",
       " 'founders',\n",
       " 'location',\n",
       " 'major_shareholder_of',\n",
       " 'major_shareholders',\n",
       " 'nationality',\n",
       " 'neighborhood_of',\n",
       " 'people',\n",
       " 'place_founded',\n",
       " 'place_lived',\n",
       " 'place_of_birth',\n",
       " 'place_of_death',\n",
       " 'religion',\n",
       " 'teams'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a7ec530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tokens', 'spo_list', 'pos_tags', 'relations'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0].keys()  # Display the keys of the first test example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de6e1fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Iowa', '/location/location/contains', 'Des Moines']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[2]['spo_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7440dfd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'h': {'text': 'Bobby Fischer', 'start': 14, 'end': 16, 'type': 'PERSON'},\n",
       "  't': {'text': 'Iceland', 'start': 35, 'end': 36, 'type': 'LOCATION'},\n",
       "  'type': '/people/person/nationality'},\n",
       " {'h': {'text': 'Iceland', 'start': 35, 'end': 36, 'type': 'LOCATION'},\n",
       "  't': {'text': 'Reykjavik', 'start': 33, 'end': 34, 'type': 'LOCATION'},\n",
       "  'type': '/location/country/capital'},\n",
       " {'h': {'text': 'Iceland', 'start': 35, 'end': 36, 'type': 'LOCATION'},\n",
       "  't': {'text': 'Reykjavik', 'start': 33, 'end': 34, 'type': 'LOCATION'},\n",
       "  'type': '/location/location/contains'},\n",
       " {'h': {'text': 'Bobby Fischer', 'start': 14, 'end': 16, 'type': 'PERSON'},\n",
       "  't': {'text': 'Reykjavik', 'start': 33, 'end': 34, 'type': 'LOCATION'},\n",
       "  'type': '/people/deceased_person/place_of_death'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0]['relations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0bc7c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'people', 'person', 'nationality']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=test[0]['spo_list'][0][1]\n",
    "s.split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9649bf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_labels=set()\n",
    "true=[]\n",
    "entity_labels_true_count={}\n",
    "for i in range(len(test)):\n",
    "    temp=[]\n",
    "    for j in test[i]['spo_list']:\n",
    "        s=j[1].split('/')\n",
    "        if s[1] not in entity_labels_true_count:\n",
    "            entity_labels_true_count[s[1]]=0\n",
    "        if s[2] not in entity_labels_true_count:\n",
    "            entity_labels_true_count[s[2]]=0\n",
    "        entity_labels_true_count[s[1]]+=1\n",
    "        entity_labels_true_count[s[2]]+=1\n",
    "        ent_labels.add(s[1])\n",
    "        ent_labels.add(s[2]) \n",
    "        temp.append(j[0])\n",
    "        temp.append(j[2])\n",
    "    true.append(temp) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3664688e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'people': 1657,\n",
       " 'person': 1955,\n",
       " 'location': 10380,\n",
       " 'country': 1288,\n",
       " 'deceased_person': 136,\n",
       " 'administrative_division': 580,\n",
       " 'business': 604,\n",
       " 'neighborhood': 394,\n",
       " 'company': 135,\n",
       " 'sports': 34,\n",
       " 'sports_team_location': 17,\n",
       " 'sports_team': 17,\n",
       " 'company_shareholder': 33,\n",
       " 'ethnicity': 2}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_labels_true_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eff4486f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['location',\n",
       " 'place_of_death',\n",
       " 'place_lived',\n",
       " 'nationality',\n",
       " 'advisors',\n",
       " 'neighborhood_of',\n",
       " 'founders',\n",
       " 'major_shareholder_of',\n",
       " 'place_founded',\n",
       " 'people',\n",
       " 'children',\n",
       " 'religion',\n",
       " 'teams',\n",
       " 'capital',\n",
       " 'country',\n",
       " 'contains',\n",
       " 'administrative_divisions',\n",
       " 'place_of_birth',\n",
       " 'company',\n",
       " 'major_shareholders']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_set=list(rel_set)\n",
    "rel_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ddc92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [1:09:59<00:00,  1.19it/s]\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd  # make sure df is defined with 'Content' column\n",
    "\n",
    "# Set OpenAI API key\n",
    "client = OpenAI(api_key = \"\")  # Replace with your actual API key\n",
    "\n",
    "# Function: extract named_entities given allowed entity labels\n",
    "def extract_named_entities_by_labels(paragraph, allowed_labels):\n",
    "    allowed_labels_str = \", \".join(allowed_labels)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a named entity recognition (NER) assistant. \"\n",
    "        \"Your task is to extract named entities from a given paragraph, \"\n",
    "        \"but only include entities whose type is one of the following: \"\n",
    "        f\"{allowed_labels_str}. \"\n",
    "        \"Return ONLY a JSON list of entity strings that match the allowed labels. \"\n",
    "        \"Do not include the labels in the output. Keep it JSON parsable.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "Paragraph:\n",
    "\n",
    "{paragraph}\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  # or \"gpt-4o-mini\"\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "        )\n",
    "\n",
    "        reply = response.choices[0].message.content.strip()\n",
    "\n",
    "        if reply.startswith(\"```json\"):\n",
    "            reply = reply[len(\"```json\"):].strip()\n",
    "        if reply.endswith(\"```\"):\n",
    "            reply = reply[:-len(\"```\")].strip()\n",
    "\n",
    "        return json.loads(reply)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error with paragraph:\", paragraph)\n",
    "        print(\"Exception:\", e)\n",
    "        return []\n",
    "\n",
    "    # Example: allowed labels\n",
    "allowed_entity_labels = ['company', 'country', 'location', 'neighborhood', 'people', 'person', 'sports']\n",
    "\n",
    "# Loop through DataFrame and extract entities\n",
    "gpt_named_entities = []\n",
    "\n",
    "for i in tqdm(range(len(test))):\n",
    "    paragraph = ' '.join(test[i]['tokens'])\n",
    "    entities = extract_named_entities_by_labels(paragraph, allowed_entity_labels)\n",
    "    gpt_named_entities.append(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b96921e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nyt_gpt_named_entities_output.json\", \"w\") as f:\n",
    "    json.dump(gpt_named_entities, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a98b7267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bobby Fischer', 'Reykjavik', 'Iceland']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_named_entities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22eee58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_avg=[]\n",
    "precision_avg=[]\n",
    "common2=0\n",
    "for i in range(len(gpt_named_entities)):\n",
    "    true_set=set(true[i])\n",
    "    pred_set=set(gpt_named_entities[i])\n",
    "    \n",
    "    if len(true_set) == 0 and len(pred_set) == 0:\n",
    "        recall_avg.append(1.0)\n",
    "        precision_avg.append(1.0)\n",
    "    else:\n",
    "        count=0\n",
    "        for m in true_set:\n",
    "            for n in pred_set:\n",
    "                if m in n or n==m:\n",
    "                    count+= 1\n",
    "        recall = count / len(true_set) if len(true_set) > 0 else 0\n",
    "        precision = count / len(pred_set) if len(pred_set) > 0 else 0\n",
    "        common2 += count\n",
    "        recall_avg.append(recall)\n",
    "        precision_avg.append(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ba93440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8971235714285717, 0.5872980432312767)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(recall_avg) / len(recall_avg), sum(precision_avg) / len(precision_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8219b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(example):\n",
    "    ner=[]\n",
    "    for ent in example['relations']:\n",
    "        head=ent['h']\n",
    "        tail=ent['t']    \n",
    "        text=head['text']\n",
    "        start=head['start']\n",
    "        end=head['end']\n",
    "        ent_type=head['type']\n",
    "        ner.append([start, end-1, ent_type, text])\n",
    "        text=tail['text']\n",
    "        start=tail['start']\n",
    "        end=tail['end']\n",
    "        ent_type=tail['type']\n",
    "        ner.append([start, end-1, ent_type, text])\n",
    "\n",
    "    example['ner'] = ner  # Add ner to the original example\n",
    "    return example\n",
    "processed_data = [process(example) for example in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a1498fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'h': {'text': 'Bobby Fischer', 'start': 14, 'end': 16, 'type': 'PERSON'},\n",
       "  't': {'text': 'Iceland', 'start': 35, 'end': 36, 'type': 'LOCATION'},\n",
       "  'type': '/people/person/nationality'},\n",
       " {'h': {'text': 'Iceland', 'start': 35, 'end': 36, 'type': 'LOCATION'},\n",
       "  't': {'text': 'Reykjavik', 'start': 33, 'end': 34, 'type': 'LOCATION'},\n",
       "  'type': '/location/country/capital'},\n",
       " {'h': {'text': 'Iceland', 'start': 35, 'end': 36, 'type': 'LOCATION'},\n",
       "  't': {'text': 'Reykjavik', 'start': 33, 'end': 34, 'type': 'LOCATION'},\n",
       "  'type': '/location/location/contains'},\n",
       " {'h': {'text': 'Bobby Fischer', 'start': 14, 'end': 16, 'type': 'PERSON'},\n",
       "  't': {'text': 'Reykjavik', 'start': 33, 'end': 34, 'type': 'LOCATION'},\n",
       "  'type': '/people/deceased_person/place_of_death'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data[0]['relations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70457c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_true=[]\n",
    "relation_labels_true_count={}\n",
    "for i in range(len(processed_data)):\n",
    "    temp=[]\n",
    "    for j in processed_data[i]['relations']:\n",
    "        s=j['type']\n",
    "        rel_type = s.split('/')[3]\n",
    "        temp.append([j['h']['text'], j['t']['text'], rel_type])\n",
    "        if rel_type not in relation_labels_true_count:\n",
    "            relation_labels_true_count[rel_type]=0\n",
    "        relation_labels_true_count[rel_type]+=1\n",
    "    relations_true.append(temp)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f67ac656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nationality': 589,\n",
       " 'capital': 708,\n",
       " 'contains': 4059,\n",
       " 'place_of_death': 136,\n",
       " 'children': 42,\n",
       " 'place_of_birth': 270,\n",
       " 'place_lived': 612,\n",
       " 'administrative_divisions': 580,\n",
       " 'country': 580,\n",
       " 'company': 436,\n",
       " 'neighborhood_of': 394,\n",
       " 'place_founded': 36,\n",
       " 'founders': 63,\n",
       " 'teams': 17,\n",
       " 'location': 17,\n",
       " 'major_shareholder_of': 33,\n",
       " 'major_shareholders': 33,\n",
       " 'people': 1,\n",
       " 'ethnicity': 1,\n",
       " 'advisors': 3,\n",
       " 'religion': 5,\n",
       " 'geographic_distribution': 1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_labels_true_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2295376f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Bobby Fischer', 'Iceland', 'nationality'],\n",
       " ['Iceland', 'Reykjavik', 'capital'],\n",
       " ['Iceland', 'Reykjavik', 'contains'],\n",
       " ['Bobby Fischer', 'Reykjavik', 'place_of_death']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations_true[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24db72af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [1:08:47<00:00,  1.21it/s]\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key = \"\")  # Replace with your actual API key\n",
    "\n",
    "def extract_relation_labels_with_gpt_entities(paragraph, entities, relation_labels):\n",
    "    \"\"\"\n",
    "    Extract RDF triples using GPT-4o/mini with entity spans.\n",
    "\n",
    "    Args:\n",
    "        paragraph (str): Full input paragraph\n",
    "        entities (list): Each entity as [start, end, entity_type, entity_text]\n",
    "        relation_labels (list): Allowed relation labels\n",
    "\n",
    "    Returns:\n",
    "        dict: {\"relation_triples\": [[head, relation, tail], ...]}\n",
    "    \"\"\"\n",
    "    # Convert entity structure into readable text\n",
    "    entity_descs = [\n",
    "        f\"[{start}, {end}, {etype}, {text}]\"\n",
    "        for start, end, etype, text in entities\n",
    "    ]\n",
    "    entity_str = \"\\n\".join(entity_descs)\n",
    "    relation_str = \", \".join(relation_labels)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are an expert in information extraction. \"\n",
    "        \"Given a paragraph, a list of named entities with character spans and types, and a list of allowed relation labels, \"\n",
    "        \"extract RDF relation triples in the format [head, relation, tail].\\n\\n\"\n",
    "        \"- Head and tail must be from the provided entity list.\\n\"\n",
    "        \"- The relation must be from the relation_labels list.\\n\"\n",
    "        \"- The output must be ONLY a JSON object like:\\n\"\n",
    "        '{ \"relation_triples\": [ [\"Entity1\", \"Relation\", \"Entity2\"], ... ] }\\n'\n",
    "        \"- Do NOT include any explanation or extra text.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "Paragraph:\n",
    "\\\"\\\"\\\"\n",
    "{paragraph}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Entities (format: [start, end, type, entity]):\n",
    "{entity_str}\n",
    "\n",
    "Relation labels:\n",
    "{relation_str}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  # or \"gpt-4o-mini\"\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "        )\n",
    "\n",
    "        reply = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Extract JSON block\n",
    "        json_match = re.search(r'\\{.*\\}', reply, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_text = json_match.group(0).strip()\n",
    "            return json.loads(json_text)\n",
    "        else:\n",
    "            print(\"No JSON found.\")\n",
    "            print(\"Reply:\", reply)\n",
    "            return {\"relation_triples\": []}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Exception:\", e)\n",
    "        return {\"relation_triples\": []}\n",
    "gpt_relation_triples = []\n",
    "relation_labels = ['location',\n",
    " 'place_of_death',\n",
    " 'place_lived',\n",
    " 'nationality',\n",
    " 'advisors',\n",
    " 'neighborhood_of',\n",
    " 'founders',\n",
    " 'major_shareholder_of',\n",
    " 'place_founded',\n",
    " 'people',\n",
    " 'children',\n",
    " 'religion',\n",
    " 'teams',\n",
    " 'capital',\n",
    " 'country',\n",
    " 'contains',\n",
    " 'administrative_divisions',\n",
    " 'place_of_birth',\n",
    " 'company',\n",
    " 'major_shareholders']\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(processed_data))):\n",
    "    paragraph = ' '.join(processed_data[i]['tokens'])\n",
    "    entities = processed_data[i]['ner']\n",
    "    output= extract_relation_labels_with_gpt_entities(paragraph, entities, relation_labels)\n",
    "    gpt_relation_triples.append(output['relation_triples'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "56acf877",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nyt_gpt_relation_triples_output.json\", \"w\") as f:\n",
    "    json.dump(gpt_relation_triples, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c5a92a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'relation_triples': [['Jersey City', 'neighborhood_of', 'New Jersey']]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f32c689",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_avg=[]\n",
    "precision_avg=[]\n",
    "common=0\n",
    "ours=0\n",
    "\n",
    "for i in range(len(relations_true)):\n",
    "    true_set=relations_true[i]\n",
    "    pred_set=gpt_relation_triples[i]\n",
    "    d={}\n",
    "    for item in pred_set:\n",
    "        if (item[0],item[1]) not in d and (item[1],item[0]) not in d:\n",
    "            d[(item[0],item[1])]=item[2]\n",
    "    \n",
    "    ours+=len(d)\n",
    "    if len(true_set) == 0 and len(pred_set) == 0:\n",
    "        recall_avg.append(1.0)\n",
    "        precision_avg.append(1.0)\n",
    "    else:\n",
    "        count=0\n",
    "        for m in true_set:\n",
    "            for n in pred_set:\n",
    "                if m[0]==n[0] and m[1]==n[2] and (m[2]==n[1]):\n",
    "                    count+= 1\n",
    "        recall = count/ len(true_set) if len(true_set) > 0 else 0\n",
    "        precision = count / len(pred_set) if len(d) > 0 else 0\n",
    "        common += count\n",
    "        recall_avg.append(recall)\n",
    "        precision_avg.append(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6aba707d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.16340497835497841, 0.20436055555555557)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(recall_avg) / len(recall_avg), sum(precision_avg) / len(precision_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6913a3",
   "metadata": {},
   "source": [
    "65.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e4aeb14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bt19d200/NER_Vamshi/NER_Model/.conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "100%|██████████| 5000/5000 [07:04<00:00, 11.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pair2rel import Pair2Rel\n",
    "\n",
    "from tqdm import tqdm\n",
    "model = Pair2Rel.from_pretrained(\"chapalavamshi022/pair2rel\")\n",
    "import torch\n",
    "\n",
    "# Force usage of GPU 1\n",
    "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.device = device \n",
    "relations_all=[]\n",
    "labels = ['contains','includes','place_lived','nationality','company','capital','neighborhood_of','place_of_birth','country',\n",
    "            'place_of_death','place_founded']\n",
    "relation_labels_predicted_count={}\n",
    "for i in tqdm(range(len(processed_data))):\n",
    "    # required_labels = []\n",
    "    # for token in processed_data[i]['tokens']:\n",
    "    #     if token in rel_set:\n",
    "    #         required_labels.append(token)\n",
    "    try:\n",
    "    \n",
    "        relations = model.predict_relations(processed_data[i]['tokens'], labels, threshold=0.0, ner=pred_ner[i], top_k=1)\n",
    "\n",
    "        sorted_data_desc = sorted(relations, key=lambda x: x['score'], reverse=True)\n",
    "        temp=[]\n",
    "        for item in sorted_data_desc:\n",
    "            head=' '.join(item['head_text'])\n",
    "            tail=' '.join(item['tail_text'])\n",
    "            if head == tail:\n",
    "                continue\n",
    "            if item['label'] not in relation_labels_predicted_count:\n",
    "                relation_labels_predicted_count[item['label']]=0\n",
    "            relation_labels_predicted_count[item['label']]+=1\n",
    "            temp.append([head,tail,item['label']])\n",
    "\n",
    "        relations_all.append(temp)\n",
    "    except:\n",
    "        relations_all.append([])\n",
    "\n",
    "print(\"Success! ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "62e6ea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_avg=[]\n",
    "precision_avg=[]\n",
    "common=0\n",
    "ours=0\n",
    "\n",
    "for i in range(len(relations_all)):\n",
    "    true_set=relations_true[i]\n",
    "    pred_set=relations_all[i]\n",
    "    d={}\n",
    "    for item in pred_set:\n",
    "        if (item[0],item[1]) not in d and (item[1],item[0]) not in d:\n",
    "            d[(item[0],item[1])]=item[2]\n",
    "    \n",
    "    ours+=len(d)\n",
    "    if len(true_set) == 0 and len(pred_set) == 0:\n",
    "        recall_avg.append(1.0)\n",
    "        precision_avg.append(1.0)\n",
    "    else:\n",
    "        count=0\n",
    "        for m in true_set:\n",
    "            for n in pred_set:\n",
    "                if (m[0] in n[0] or n[0] in m[0]) and (m[1] in n[1] or n[1] in m[1]) and (m[2]==n[2] or (m[2]=='administrative_divisions' and n[2]=='country') or (m[2]=='contains' and n[2]=='includes') or (m[2]=='country' and n[2]=='capital')):\n",
    "                    count+= 1\n",
    "        recall = count/ len(true_set) if len(true_set) > 0 else 0\n",
    "        precision = count / len(d) if len(d) > 0 else 0\n",
    "        common += count\n",
    "        recall_avg.append(recall)\n",
    "        precision_avg.append(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "79a2ee4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3301430772005775, 0.19016944166944294)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(recall_avg) / len(recall_avg), sum(precision_avg) / len(precision_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16f5e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bt19d200/NER_Vamshi/NER_Model/.conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "100%|██████████| 5000/5000 [05:23<00:00, 15.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pair2rel import Pair2Rel\n",
    "\n",
    "from tqdm import tqdm\n",
    "model = Pair2Rel.from_pretrained(\"chapalavamshi022/pair2rel\")\n",
    "import torch\n",
    "\n",
    "# Force usage of GPU 1\n",
    "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.device = device \n",
    "relations_all=[]\n",
    "labels = ['contains','includes','place_lived','nationality','company','capital','neighborhood_of','place_of_birth','country',\n",
    "            'place_of_death','place_founded']\n",
    "relation_labels_predicted_count={}\n",
    "for i in tqdm(range(len(processed_data))):\n",
    "    # required_labels = []\n",
    "    # for token in processed_data[i]['tokens']:\n",
    "    #     if token in rel_set:\n",
    "    #         required_labels.append(token)\n",
    "    \n",
    "    relations = model.predict_relations(processed_data[i]['tokens'], labels, threshold=0.0, ner=processed_data[i]['ner'], top_k=2)\n",
    "\n",
    "    sorted_data_desc = sorted(relations, key=lambda x: x['score'], reverse=True)\n",
    "    temp=[]\n",
    "    for item in sorted_data_desc:\n",
    "        head=' '.join(item['head_text'])\n",
    "        tail=' '.join(item['tail_text'])\n",
    "        if head == tail:\n",
    "            continue\n",
    "        if item['label'] not in relation_labels_predicted_count:\n",
    "            relation_labels_predicted_count[item['label']]=0\n",
    "        relation_labels_predicted_count[item['label']]+=1\n",
    "        temp.append([head,tail,item['label']])\n",
    "\n",
    "    relations_all.append(temp)\n",
    "        \n",
    "print(\"Success! ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5c67172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_avg=[]\n",
    "precision_avg=[]\n",
    "common=0\n",
    "ours=0\n",
    "\n",
    "for i in range(len(relations_all)):\n",
    "    true_set=relations_true[i]\n",
    "    pred_set=relations_all[i]\n",
    "    d={}\n",
    "    for item in pred_set:\n",
    "        if (item[0],item[1]) not in d and (item[1],item[0]) not in d:\n",
    "            d[(item[0],item[1])]=item[2]\n",
    "    \n",
    "    ours+=len(d)\n",
    "    if len(true_set) == 0 and len(pred_set) == 0:\n",
    "        recall_avg.append(1.0)\n",
    "        precision_avg.append(1.0)\n",
    "    else:\n",
    "        count=0\n",
    "        for m in true_set:\n",
    "            for n in pred_set:\n",
    "                if m[0]==n[0] and m[1]==n[1] and (m[2]==n[2] or (m[2]=='administrative_divisions' and n[2]=='country') or (m[2]=='contains' and n[2]=='includes') or (m[2]=='country' and n[2]=='capital')):\n",
    "                    count+= 1\n",
    "        recall = count/ len(true_set) if len(true_set) > 0 else 0\n",
    "        precision = count / len(pred_set) if len(d) > 0 else 0\n",
    "        common += count\n",
    "        recall_avg.append(recall)\n",
    "        precision_avg.append(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c9dbf6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6480175036075041, 0.2525411111111112)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(recall_avg) / len(recall_avg), sum(precision_avg) / len(precision_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bf48ad38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bt19d200/NER_Vamshi/NER_Model/.conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "100%|██████████| 5000/5000 [05:14<00:00, 15.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pair2rel import Pair2Rel\n",
    "\n",
    "from tqdm import tqdm\n",
    "model = Pair2Rel.from_pretrained(\"chapalavamshi022/pair2rel\")\n",
    "import torch\n",
    "\n",
    "# Force usage of GPU 1\n",
    "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.device = device \n",
    "relations_all=[]\n",
    "labels = ['contains','includes','place_lived','nationality','company','capital','neighborhood_of','place_of_birth','country',\n",
    "            'place_of_death','place_founded']\n",
    "relation_labels_predicted_count={}\n",
    "for i in tqdm(range(len(processed_data))):\n",
    "    # required_labels = []\n",
    "    # for token in processed_data[i]['tokens']:\n",
    "    #     if token in rel_set:\n",
    "    #         required_labels.append(token)\n",
    "    \n",
    "    relations = model.predict_relations(processed_data[i]['tokens'], labels, threshold=0.0, ner=processed_data[i]['ner'], top_k=3)\n",
    "\n",
    "    sorted_data_desc = sorted(relations, key=lambda x: x['score'], reverse=True)\n",
    "    temp=[]\n",
    "    for item in sorted_data_desc:\n",
    "        head=' '.join(item['head_text'])\n",
    "        tail=' '.join(item['tail_text'])\n",
    "        if head == tail:\n",
    "            continue\n",
    "        if item['label'] not in relation_labels_predicted_count:\n",
    "            relation_labels_predicted_count[item['label']]=0\n",
    "        relation_labels_predicted_count[item['label']]+=1\n",
    "        temp.append([head,tail,item['label']])\n",
    "\n",
    "    relations_all.append(temp)\n",
    "        \n",
    "\n",
    "print(\"Success! ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "95c16757",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_avg=[]\n",
    "precision_avg=[]\n",
    "common=0\n",
    "ours=0\n",
    "\n",
    "for i in range(len(relations_all)):\n",
    "    true_set=relations_true[i]\n",
    "    pred_set=relations_all[i]\n",
    "    d={}\n",
    "    for item in pred_set:\n",
    "        if (item[0],item[1]) not in d and (item[1],item[0]) not in d:\n",
    "            d[(item[0],item[1])]=item[2]\n",
    "    \n",
    "    ours+=len(d)\n",
    "    if len(true_set) == 0 and len(pred_set) == 0:\n",
    "        recall_avg.append(1.0)\n",
    "        precision_avg.append(1.0)\n",
    "    else:\n",
    "        count=0\n",
    "        for m in true_set:\n",
    "            for n in pred_set:\n",
    "                if m[0]==n[0] and m[1]==n[1] and (m[2]==n[2] or (m[2]=='administrative_divisions' and n[2]=='country') or (m[2]=='contains' and n[2]=='includes') or (m[2]=='country' and n[2]=='capital')):\n",
    "                    count+= 1\n",
    "        recall = count/ len(true_set) if len(true_set) > 0 else 0\n",
    "        precision = count / len(pred_set) if len(d) > 0 else 0\n",
    "        common += count\n",
    "        recall_avg.append(recall)\n",
    "        precision_avg.append(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "510dc260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8932503030303031, 0.23985740740740738)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(recall_avg) / len(recall_avg), sum(precision_avg) / len(precision_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10855a20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
