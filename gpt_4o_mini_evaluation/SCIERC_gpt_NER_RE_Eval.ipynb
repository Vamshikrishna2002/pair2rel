{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efe17c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0712c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "with open('scierc_data.json', 'r') as file:\n",
    "    for lines in file:\n",
    "        # Process each line as needed\n",
    "        # For example, if each line is a JSON object, you can parse it\n",
    "        data.append(json.loads(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "394db6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = []\n",
    "for i in range(len(data)):\n",
    "    cnt=0\n",
    "    for j in range(len(data[i]['sentences'])):\n",
    "        ner=[]\n",
    "        for k in range(len(data[i]['ner'][j])):\n",
    "            if data[i]['ner'][j][k]:\n",
    "                \n",
    "                start = data[i]['ner'][j][k][0]-cnt\n",
    "                end= data[i]['ner'][j][k][1]-cnt\n",
    "                word=' '.join(data[i]['sentences'][j][start:end+1])\n",
    "                ner.append([start, end, word,data[i]['ner'][j][k][2]])\n",
    "        relations=[]\n",
    "        for k in range(len(data[i]['relations'][j])):\n",
    "            if data[i]['relations'][j][k]:\n",
    "                start = data[i]['relations'][j][k][0]-cnt\n",
    "                end= data[i]['relations'][j][k][1]-cnt\n",
    "                word1=' '.join(data[i]['sentences'][j][start:end+1])\n",
    "                start = data[i]['relations'][j][k][2]-cnt\n",
    "                end= data[i]['relations'][j][k][3]-cnt\n",
    "                word2=' '.join(data[i]['sentences'][j][start:end+1])\n",
    "                relations.append([word1, word2, data[i]['relations'][j][k][4]])\n",
    "        cnt+=len(data[i]['sentences'][j])\n",
    "            \n",
    "        new_data.append({\n",
    "            'sentence': data[i]['sentences'][j],\n",
    "            'ner': ner,\n",
    "            'relations': relations\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4878038",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_labels=set()\n",
    "entity_labels_true_count={}\n",
    "other={}\n",
    "for i in range(len(new_data)):\n",
    "    for j in new_data[i]['ner']:\n",
    "        if j[3] not in entity_labels_true_count:\n",
    "            entity_labels_true_count[j[3]] = 0\n",
    "        if j[3]=='OtherScientificTerm':\n",
    "            if j[2] not in other:\n",
    "                other[j[2]]=0\n",
    "            other[j[2]]+=1\n",
    "        entity_labels_true_count[j[3]] += 1\n",
    "        entity_labels.add(j[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b17f7956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Generic', 'Material', 'Method', 'Metric', 'OtherScientificTerm', 'Task'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f99209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 551/551 [06:45<00:00,  1.36it/s]\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd  # make sure df is defined with 'Content' column\n",
    "\n",
    "# Set OpenAI API key\n",
    "client = OpenAI(api_key = \"\")  # Replace with your actual API key\n",
    "\n",
    "# Function: extract named_entities given allowed entity labels\n",
    "def extract_named_entities_by_labels(paragraph, allowed_labels):\n",
    "    allowed_labels_str = \", \".join(allowed_labels)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a named entity recognition (NER) assistant. \"\n",
    "        \"Your task is to extract named entities from a given paragraph, \"\n",
    "        \"but only include entities whose type is one of the following: \"\n",
    "        f\"{allowed_labels_str}. \"\n",
    "        \"Return ONLY a JSON list of entity strings that match the allowed labels. \"\n",
    "        \"Do not include the labels in the output. Keep it JSON parsable.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "Paragraph:\n",
    "\n",
    "{paragraph}\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  # or \"gpt-4o-mini\"\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "        )\n",
    "\n",
    "        reply = response.choices[0].message.content.strip()\n",
    "\n",
    "        if reply.startswith(\"```json\"):\n",
    "            reply = reply[len(\"```json\"):].strip()\n",
    "        if reply.endswith(\"```\"):\n",
    "            reply = reply[:-len(\"```\")].strip()\n",
    "\n",
    "        return json.loads(reply)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error with paragraph:\", paragraph)\n",
    "        print(\"Exception:\", e)\n",
    "        return []\n",
    "\n",
    "    # Example: allowed labels\n",
    "allowed_entity_labels = ['Generic', 'Material', 'Method', 'Metric', 'OtherScientificTerm', 'Task']\n",
    "\n",
    "# Loop through DataFrame and extract entities\n",
    "gpt_named_entities = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(new_data))):\n",
    "    paragraph = ' '.join(new_data[i]['sentence'])\n",
    "    entities = extract_named_entities_by_labels(paragraph, allowed_entity_labels)\n",
    "    gpt_named_entities.append(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7e4bddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"SCIERC_gpt_named_entities_output.json\", \"w\") as f:\n",
    "    json.dump(gpt_named_entities, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d48e594",
   "metadata": {},
   "outputs": [],
   "source": [
    "true=[]\n",
    "for i in range(len(new_data)):\n",
    "    temp=[]\n",
    "    for j in new_data[i]['ner']:\n",
    "        temp.append(j[2])\n",
    "    true.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81774df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['approach',\n",
       " 'Multi-lingual Evaluation Task -LRB- MET -RRB-',\n",
       " 'Japanese text',\n",
       " 'task',\n",
       " 'morphological analysis problem',\n",
       " 'Japanese']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a544bb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "their=0\n",
    "ours=0\n",
    "common=0\n",
    "for i in range(len(gpt_named_entities)):\n",
    "    their+=len(set(true[i]))\n",
    "    ours+=len(set(gpt_named_entities[i]))\n",
    "    common+=len(set(true[i]).intersection(set(gpt_named_entities[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bd725a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1673, 584, 353)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "their,ours,common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb0e708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_avg=[]\n",
    "precision_avg=[]\n",
    "common2=0\n",
    "for i in range(len(gpt_named_entities)):\n",
    "    true_set=set(true[i])\n",
    "    pred_set=set(gpt_named_entities[i])\n",
    "    \n",
    "    if len(true_set) == 0 and len(pred_set) == 0:\n",
    "        recall_avg.append(1.0)\n",
    "        precision_avg.append(1.0)\n",
    "    else:\n",
    "        count=0\n",
    "        for m in true_set:\n",
    "            for n in pred_set:\n",
    "                if m in n or n in m:\n",
    "                    count+= 1\n",
    "        recall = count / len(true_set) if len(true_set) > 0 else 0\n",
    "        precision = count / len(pred_set) if len(pred_set) > 0 else 0\n",
    "        common2 += count\n",
    "        recall_avg.append(recall)\n",
    "        precision_avg.append(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd9f954c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "213bb223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.29432533588613446, 0.30358223144067076)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(recall_avg) / len(recall_avg), sum(precision_avg) / len(precision_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80c4c08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(new_data)):\n",
    "    for j in range(len(new_data[i]['ner'])):\n",
    "        new_data[i]['ner'][j][2],new_data[i]['ner'][j][3] = new_data[i]['ner'][j][3], new_data[i]['ner'][j][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8e50cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final={}\n",
    "rel_count={}\n",
    "for i in range(len(new_data)):\n",
    "    mapp={}\n",
    "    for j in new_data[i]['ner']:\n",
    "        mapp[j[3]]=j[2]\n",
    "    for j in new_data[i]['relations']:\n",
    "        if j[2] not in rel_count:\n",
    "            rel_count[j[2]] = 0\n",
    "        rel_count[j[2]] += 1\n",
    "        if j[2] not in final:\n",
    "            final[j[2]] = set()\n",
    "        final[j[2]].add((mapp[j[0]], mapp[j[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5af8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 551/551 [10:03<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key = \"\")  # Replace with your actual API key\n",
    "\n",
    "def extract_relation_labels_with_gpt_entities(paragraph, entities, relation_labels):\n",
    "    \"\"\"\n",
    "    Extract RDF triples using GPT-4o/mini with entity spans.\n",
    "\n",
    "    Args:\n",
    "        paragraph (str): Full input paragraph\n",
    "        entities (list): Each entity as [start, end, entity_type, entity_text]\n",
    "        relation_labels (list): Allowed relation labels\n",
    "\n",
    "    Returns:\n",
    "        dict: {\"relation_triples\": [[head, relation, tail], ...]}\n",
    "    \"\"\"\n",
    "    # Convert entity structure into readable text\n",
    "    entity_descs = [\n",
    "        f\"[{start}, {end}, {etype}, {text}]\"\n",
    "        for start, end, etype, text in entities\n",
    "    ]\n",
    "    entity_str = \"\\n\".join(entity_descs)\n",
    "    relation_str = \", \".join(relation_labels)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are an expert in information extraction. \"\n",
    "        \"Given a paragraph, a list of named entities with character spans and types, and a list of allowed relation labels, \"\n",
    "        \"extract RDF relation triples in the format [head, relation, tail].\\n\\n\"\n",
    "        \"- Head and tail must be from the provided entity list.\\n\"\n",
    "        \"- The relation must be from the relation_labels list.\\n\"\n",
    "        \"- The output must be ONLY a JSON object like:\\n\"\n",
    "        '{ \"relation_triples\": [ [\"Entity1\", \"Relation\", \"Entity2\"], ... ] }\\n'\n",
    "        \"- Do NOT include any explanation or extra text.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "Paragraph:\n",
    "\\\"\\\"\\\"\n",
    "{paragraph}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Entities (format: [start, end, type, entity]):\n",
    "{entity_str}\n",
    "\n",
    "Relation labels:\n",
    "{relation_str}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  # or \"gpt-4o-mini\"\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "        )\n",
    "\n",
    "        reply = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Extract JSON block\n",
    "        json_match = re.search(r'\\{.*\\}', reply, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_text = json_match.group(0).strip()\n",
    "            return json.loads(json_text)\n",
    "        else:\n",
    "            print(\"No JSON found.\")\n",
    "            print(\"Reply:\", reply)\n",
    "            return {\"relation_triples\": []}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Exception:\", e)\n",
    "        return {\"relation_triples\": []}\n",
    "gpt_relation_triples = []\n",
    "relation_labels = ['USED-FOR','CONJUNCTION', 'EVALUATE-FOR', 'FEATURE-OF', 'PART-OF','COMPARE']\n",
    "\n",
    "for i in tqdm(range(len(new_data))):\n",
    "    paragraph = ' '.join(new_data[i]['sentence'])\n",
    "    entities = new_data[i]['ner']\n",
    "    output= extract_relation_labels_with_gpt_entities(paragraph, entities, relation_labels)\n",
    "    gpt_relation_triples.append(output['relation_triples'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d9016c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"SCIERC_gpt_relation_triples_output.json\", \"w\") as f:\n",
    "    json.dump(gpt_relation_triples, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "617e1158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['approach', 'USED-FOR', 'Multi-lingual Evaluation Task -LRB- MET -RRB-'],\n",
       " ['Multi-lingual Evaluation Task -LRB- MET -RRB-',\n",
       "  'EVALUATE-FOR',\n",
       "  'Japanese text'],\n",
       " ['task', 'FEATURE-OF', 'morphological analysis problem'],\n",
       " ['morphological analysis problem', 'PART-OF', 'Japanese']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_relation_triples[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e3f503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_true=[]\n",
    "for i in range(len(new_data)):\n",
    "    \n",
    "    relations_true.append(new_data[i]['relations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "28844e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['approach', 'Multi-lingual Evaluation Task -LRB- MET -RRB-', 'USED-FOR'],\n",
       " ['Multi-lingual Evaluation Task -LRB- MET -RRB-',\n",
       "  'Japanese text',\n",
       "  'USED-FOR'],\n",
       " ['morphological analysis problem', 'task', 'USED-FOR'],\n",
       " ['Japanese', 'morphological analysis problem', 'USED-FOR']]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations_true[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ebe5395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_avg=[]\n",
    "precision_avg=[]\n",
    "common=0\n",
    "ours=0\n",
    "relations_all_filtered=[]\n",
    "\n",
    "for i in range(len(relations_true)):\n",
    "    true_set=relations_true[i]\n",
    "    pred_set=gpt_relation_triples[i]\n",
    "    d={}\n",
    "    temp=[]\n",
    "    for item in pred_set:\n",
    "        if (item[0],item[2]) not in d and (item[2],item[0]) not in d:\n",
    "            d[(item[0],item[2])]=item[1]\n",
    "            temp.append([item[0],item[1],item[2]])\n",
    "    relations_all_filtered.append(temp)\n",
    "    \n",
    "    ours+=len(d)\n",
    "    if len(true_set) == 0 and len(pred_set) == 0:\n",
    "        recall_avg.append(1.0)\n",
    "        precision_avg.append(1.0)\n",
    "    else:\n",
    "        count=0\n",
    "        for m in true_set:\n",
    "            for n in pred_set:\n",
    "                if m[0]==n[0] and m[1]==n[2] and (m[2]==n[1]):\n",
    "                    count+= 1\n",
    "        recall = count/ len(true_set) if len(true_set) > 0 else 0\n",
    "        precision = count / len(pred_set) if len(pred_set) > 0 else 0\n",
    "        common += count\n",
    "        recall_avg.append(recall)\n",
    "        precision_avg.append(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9df44f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.42119097377718073, 0.4164419669864318)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(recall_avg) / len(recall_avg), sum(precision_avg) / len(precision_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec36701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
