{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efe17c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bt19d200/NER_Vamshi/NER_Model/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0712c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=load_dataset(\"DFKI-SLT/conll04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4878038",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_labels=set()\n",
    "for i in range(len(dataset['test'])):\n",
    "    for j in dataset['test'][i]['entities']:\n",
    "        entity_labels.add(j['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b17f7956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Loc', 'Org', 'Other', 'Peop'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d925bf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c298e862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'end': 7, 'start': 5, 'type': 'Org'},\n",
       " {'end': 9, 'start': 8, 'type': 'Other'},\n",
       " {'end': 11, 'start': 10, 'type': 'Loc'},\n",
       " {'end': 18, 'start': 17, 'type': 'Other'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0]['entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c1d549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_labels_count_true={}\n",
    "for i in range(len(dataset['test'])):\n",
    "    for j in dataset['test'][i]['entities']:\n",
    "        if j['type'] not in entity_labels_count_true:\n",
    "            entity_labels_count_true[j['type']]=0\n",
    "        entity_labels_count_true[j['type']]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3456b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner=[]\n",
    "for i in range(len(test)):\n",
    "    temp=[]\n",
    "    for j in test[i]['entities']:\n",
    "        text=' '.join(test[i]['tokens'][j['start']:j['end']])\n",
    "        temp.append([j['start'], j['end']-1,j['type'],text])\n",
    "    ner.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17c55068",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_other={}\n",
    "for i in range(len(ner)):\n",
    "    for j in ner[i]:\n",
    "        if j[2]=='Other':\n",
    "            if j[3] not in entity_other:\n",
    "                entity_other[j[3]]=0\n",
    "            entity_other[j[3]]+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4f27bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arab': 1,\n",
       " 'Palestinians': 1,\n",
       " 'American': 2,\n",
       " 'Dec. 13 ,': 1,\n",
       " 'Dec. 16 ,': 1,\n",
       " '1973': 1,\n",
       " '1974': 1,\n",
       " 'Watergate': 2,\n",
       " 'Cuban missile crisis': 1,\n",
       " 'Armenian': 1,\n",
       " 'British': 1,\n",
       " '100 million years': 1,\n",
       " '69 mph': 1,\n",
       " '57 mph': 1,\n",
       " '2 a.m.': 1,\n",
       " '9 degrees': 1,\n",
       " '85 degrees': 1,\n",
       " 'Jan. 3': 1,\n",
       " '$3.2 billion': 1,\n",
       " '$3.6 billion': 1,\n",
       " '$1 , 600 ,': 1,\n",
       " 'April 4 , 1968': 1,\n",
       " '2 million': 1,\n",
       " '8 million': 1,\n",
       " '1980': 1,\n",
       " 'Poles': 1,\n",
       " '14 , 800 feet': 1,\n",
       " '$5 million': 1,\n",
       " '1985': 1,\n",
       " 'Mexican': 1,\n",
       " '50 mph': 1,\n",
       " '10 percent': 2,\n",
       " '$60.6 million': 1,\n",
       " '3 p.m.': 1,\n",
       " 'July 31 , 1989': 1,\n",
       " '1942.': 1,\n",
       " '1942': 1,\n",
       " '15 miles': 1,\n",
       " '6 million acres': 1,\n",
       " '1988': 1,\n",
       " '15 percent': 1,\n",
       " '$100 million': 1,\n",
       " '2 a.m. EDT': 1,\n",
       " '3 a.m. EDT': 1,\n",
       " '60 , 000 acres': 1,\n",
       " '40 million': 1,\n",
       " 'March 16 , 1985.': 1,\n",
       " '2 p.m. EDT': 1,\n",
       " '7 -': 1,\n",
       " '84 percent': 1,\n",
       " '11 , 700 acres': 1,\n",
       " '28 percent': 1,\n",
       " 'March 1994': 1,\n",
       " '1924 GMT': 1,\n",
       " '21 Mar 94': 1,\n",
       " '100 kilometres': 1,\n",
       " '6 Feb 94': 1,\n",
       " '1646 GMT': 1,\n",
       " '15 Feb 94': 2,\n",
       " '2200 GMT': 1,\n",
       " '3 Feb 94': 1,\n",
       " '0900 GMT': 1,\n",
       " '1540 GMT': 1,\n",
       " '9 Feb 94': 1,\n",
       " '15 Oct 93': 1,\n",
       " '15 December': 1,\n",
       " '11 Nov 93': 1,\n",
       " '26 Dec': 1,\n",
       " '25 December': 1,\n",
       " '125 km': 1,\n",
       " 'Iodine- 131': 1,\n",
       " '19 Mar 94': 1,\n",
       " '5 Apr 94': 1,\n",
       " '23 Jun': 1,\n",
       " '37 percent': 1,\n",
       " '1200 GMT': 1,\n",
       " '28 May 94': 1,\n",
       " '23 Jun 94': 1,\n",
       " 'May 9': 1,\n",
       " '12.14AM': 1,\n",
       " '( 9 May': 1,\n",
       " '0945 GMT': 1,\n",
       " '26 May 94': 1,\n",
       " 'May 11': 1,\n",
       " 'Slovak': 1,\n",
       " '18 May': 1,\n",
       " 'May 20': 1,\n",
       " 'Russians': 1,\n",
       " '1750 GMT': 1,\n",
       " '23 May 94': 1,\n",
       " '23 5 1994': 1,\n",
       " 'Kurds': 1,\n",
       " '20 Apr 94': 1,\n",
       " 'Nov.': 5,\n",
       " 'Dallas County Administration Building': 2,\n",
       " 'Highlight': 1,\n",
       " 'June': 1,\n",
       " 'Fort Lee': 1,\n",
       " 'Wednesday': 1,\n",
       " 'Monday': 1,\n",
       " 'Tuesday': 3,\n",
       " 'November': 1,\n",
       " 'April': 3,\n",
       " 'Sic Semper Tyranus': 1,\n",
       " 'Tyrants': 1,\n",
       " 'Dec.': 1,\n",
       " 'Middle': 1,\n",
       " 'Colt Cobra': 1,\n",
       " 'Sunday': 2,\n",
       " 'National Industrial Recovery Act': 1,\n",
       " 'Universal Declaration': 1,\n",
       " 'Human Rights': 1,\n",
       " 'Friday': 1,\n",
       " 'Jan.': 1,\n",
       " 'Photo': 1,\n",
       " 'COLOR': 1,\n",
       " 'Oct.': 2,\n",
       " 'Thursday': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57196aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Org': 198, 'Other': 133, 'Loc': 427, 'Peop': 321}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_labels_count_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b334b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.86.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/bt19d200/NER_Vamshi/NER_Model/.conda/lib/python3.10/site-packages (from openai) (4.9.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/bt19d200/NER_Vamshi/NER_Model/.conda/lib/python3.10/site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Using cached jiter-0.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/bt19d200/NER_Vamshi/NER_Model/.conda/lib/python3.10/site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in /home/bt19d200/NER_Vamshi/NER_Model/.conda/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/bt19d200/NER_Vamshi/NER_Model/.conda/lib/python3.10/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/bt19d200/NER_Vamshi/NER_Model/.conda/lib/python3.10/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/bt19d200/NER_Vamshi/NER_Model/.conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/bt19d200/NER_Vamshi/NER_Model/.conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /home/bt19d200/NER_Vamshi/NER_Model/.conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /home/bt19d200/NER_Vamshi/NER_Model/.conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/bt19d200/NER_Vamshi/NER_Model/.conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/bt19d200/NER_Vamshi/NER_Model/.conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/bt19d200/NER_Vamshi/NER_Model/.conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Downloading openai-1.86.0-py3-none-any.whl (730 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m730.3/730.3 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached jiter-0.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Installing collected packages: jiter, distro, openai\n",
      "Successfully installed distro-1.9.0 jiter-0.10.0 openai-1.86.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b95df20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/288 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [03:27<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd  # make sure df is defined with 'Content' column\n",
    "\n",
    "# Set OpenAI API key\n",
    "client = OpenAI(api_key = \"\")  # Replace with your actual API key\n",
    "\n",
    "# Function: extract named_entities given allowed entity labels\n",
    "def extract_named_entities_by_labels(paragraph, allowed_labels):\n",
    "    allowed_labels_str = \", \".join(allowed_labels)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a named entity recognition (NER) assistant. \"\n",
    "        \"Your task is to extract named entities from a given paragraph, \"\n",
    "        \"but only include entities whose type is one of the following: \"\n",
    "        f\"{allowed_labels_str}. \"\n",
    "        \"Return ONLY a JSON list of entity strings that match the allowed labels. \"\n",
    "        \"Do not include the labels in the output. Keep it JSON parsable.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "Paragraph:\n",
    "\n",
    "{paragraph}\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  # or \"gpt-4o-mini\"\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "        )\n",
    "\n",
    "        reply = response.choices[0].message.content.strip()\n",
    "\n",
    "        if reply.startswith(\"```json\"):\n",
    "            reply = reply[len(\"```json\"):].strip()\n",
    "        if reply.endswith(\"```\"):\n",
    "            reply = reply[:-len(\"```\")].strip()\n",
    "\n",
    "        return json.loads(reply)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error with paragraph:\", paragraph)\n",
    "        print(\"Exception:\", e)\n",
    "        return []\n",
    "\n",
    "    # Example: allowed labels\n",
    "allowed_entity_labels = [\"Person\", \"Organization\", \"Location\", \"Other\"]\n",
    "\n",
    "# Loop through DataFrame and extract entities\n",
    "gpt_named_entities = []\n",
    "\n",
    "for i in tqdm(range(len(test))):\n",
    "    paragraph = ' '.join(test[i]['tokens'])\n",
    "    entities = extract_named_entities_by_labels(paragraph, allowed_entity_labels)\n",
    "    gpt_named_entities.append(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d3dee7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"conll04_gpt_named_entities_output.json\", \"w\") as f:\n",
    "    json.dump(gpt_named_entities, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4d48e594",
   "metadata": {},
   "outputs": [],
   "source": [
    "true=[]\n",
    "for i in range(len(test)):\n",
    "    temp=[]\n",
    "    for j in test[i]['entities']:\n",
    "        text=' '.join(test[i]['tokens'][j['start']:j['end']])\n",
    "        temp.append(text)\n",
    "    true.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "243843fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['President Nixon', 'Bush']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_named_entities[34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "81774df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Watergate', 'U.S.', 'Bush']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true[34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a544bb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "their=0\n",
    "ours=0\n",
    "common=0\n",
    "for i in range(len(gpt_named_entities)):\n",
    "    their+=len(set(true[i]))\n",
    "    ours+=len(set(gpt_named_entities[i]))\n",
    "    common+=len(set(true[i]).intersection(set(gpt_named_entities[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5bd725a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1059, 878, 692)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "their,ours,common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cb0e708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_avg=[]\n",
    "precision_avg=[]\n",
    "common2=0\n",
    "for i in range(len(gpt_named_entities)):\n",
    "    true_set=set(true[i])\n",
    "    pred_set=set(gpt_named_entities[i])\n",
    "    \n",
    "    if len(true_set) == 0 and len(pred_set) == 0:\n",
    "        recall_avg.append(1.0)\n",
    "        precision_avg.append(1.0)\n",
    "    else:\n",
    "        count=0\n",
    "        for m in true_set:\n",
    "            for n in pred_set:\n",
    "                \n",
    "                if m in n or m[:-2]==n:\n",
    "                    count+= 1\n",
    "        recall = count / len(true_set) if len(true_set) > 0 else 0\n",
    "        precision = count / len(pred_set) if len(pred_set) > 0 else 0\n",
    "        common2 += count\n",
    "        recall_avg.append(recall)\n",
    "        precision_avg.append(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fd9f954c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "746"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "213bb223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.721516754850088, 0.7958829365079366)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(recall_avg) / len(recall_avg), sum(precision_avg) / len(precision_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7ebec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/288 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [04:33<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key = \"\")  # Replace with your actual API key\n",
    "\n",
    "def extract_relation_labels_with_gpt_entities(paragraph, entities, relation_labels):\n",
    "    \"\"\"\n",
    "    Extract RDF triples using GPT-4o/mini with entity spans.\n",
    "\n",
    "    Args:\n",
    "        paragraph (str): Full input paragraph\n",
    "        entities (list): Each entity as [start, end, entity_type, entity_text]\n",
    "        relation_labels (list): Allowed relation labels\n",
    "\n",
    "    Returns:\n",
    "        dict: {\"relation_triples\": [[head, relation, tail], ...]}\n",
    "    \"\"\"\n",
    "    # Convert entity structure into readable text\n",
    "    entity_descs = [\n",
    "        f\"[{start}, {end}, {etype}, {text}]\"\n",
    "        for start, end, etype, text in entities\n",
    "    ]\n",
    "    entity_str = \"\\n\".join(entity_descs)\n",
    "    relation_str = \", \".join(relation_labels)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are an expert in information extraction. \"\n",
    "        \"Given a paragraph, a list of named entities with character spans and types, and a list of allowed relation labels, \"\n",
    "        \"extract RDF relation triples in the format [head, relation, tail].\\n\\n\"\n",
    "        \"- Head and tail must be from the provided entity list.\\n\"\n",
    "        \"- The relation must be from the relation_labels list.\\n\"\n",
    "        \"- The output must be ONLY a JSON object like:\\n\"\n",
    "        '{ \"relation_triples\": [ [\"Entity1\", \"Relation\", \"Entity2\"], ... ] }\\n'\n",
    "        \"- Do NOT include any explanation or extra text.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "Paragraph:\n",
    "\\\"\\\"\\\"\n",
    "{paragraph}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Entities (format: [start, end, type, entity]):\n",
    "{entity_str}\n",
    "\n",
    "Relation labels:\n",
    "{relation_str}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  # or \"gpt-4o-mini\"\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "        )\n",
    "\n",
    "        reply = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Extract JSON block\n",
    "        json_match = re.search(r'\\{.*\\}', reply, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_text = json_match.group(0).strip()\n",
    "            return json.loads(json_text)\n",
    "        else:\n",
    "            print(\"No JSON found.\")\n",
    "            print(\"Reply:\", reply)\n",
    "            return {\"relation_triples\": []}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Exception:\", e)\n",
    "        return {\"relation_triples\": []}\n",
    "gpt_relation_triples = []\n",
    "relation_labels = ['Located_In', 'Live_In', 'Work_For', 'Kill', 'OrgBased_In']\n",
    "\n",
    "for i in tqdm(range(len(test))):\n",
    "    paragraph = ' '.join(test[i]['tokens'])\n",
    "    entities = ner[i]\n",
    "    output= extract_relation_labels_with_gpt_entities(paragraph, entities, relation_labels)\n",
    "    gpt_relation_triples.append(output['relation_triples'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "38be296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"conll04_gpt_relation_triples_output.json\", \"w\") as f:\n",
    "    json.dump(gpt_relation_triples, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8e3f503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_true=[]\n",
    "for i in range(len(test)):\n",
    "    temp=[]\n",
    "    for j in test[i]['relations']:\n",
    "        head=ner[i][j['head']][3]\n",
    "        tail=ner[i][j['tail']][3]\n",
    "        temp.append([head, tail, j['type']])\n",
    "    relations_true.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fa7a8c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_relation_triples[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eb65efa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Reagan', 'America', 'Live_In']]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations_true[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "86aa262b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Anastas Mikoyan , then Soviet first deputy premier , was the catalyst for the reversal of Khrushchev 's order , according to the authors .\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(test[7]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "28844e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Anastas Mikoyan', 'Soviet', 'Live_In'], ['Khrushchev', 'Soviet', 'Live_In']]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations_true[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ebe5395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_avg=[]\n",
    "precision_avg=[]\n",
    "common=0\n",
    "ours=0\n",
    "relations_all_filtered=[]\n",
    "for i in range(len(relations_true)):\n",
    "    true_set=relations_true[i]\n",
    "    pred_set=gpt_relation_triples[i]\n",
    "    d={}\n",
    "    temp=[]\n",
    "    for item in pred_set:\n",
    "        if (item[0],item[2]) not in d:\n",
    "            d[(item[0],item[2])]=item[1]\n",
    "            temp.append([item[0],item[1],item[2]])\n",
    "    relations_all_filtered.append(temp)\n",
    "    \n",
    "    ours+=len(d)\n",
    "    if len(true_set) == 0 and len(pred_set) == 0:\n",
    "        recall_avg.append(1.0)\n",
    "        precision_avg.append(1.0)\n",
    "    else:\n",
    "        count=0\n",
    "        for m in true_set:\n",
    "            for n in pred_set:\n",
    "                if m[0]==n[0] and m[1]==n[2] and (m[2]==n[1]):\n",
    "                    count+= 1\n",
    "        recall = count/ len(true_set) if len(true_set) > 0 else 0\n",
    "        precision = count / len(pred_set) if len(pred_set) > 0 else 0\n",
    "        common += count\n",
    "        recall_avg.append(recall)\n",
    "        precision_avg.append(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "24d5979b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hakawati Theatre', 'Jerusalem', 'OrgBased_In']]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations_true[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0187a851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hakawati Theatre', 'Located_In', 'Jerusalem'],\n",
       " ['Palestinians', 'Kill', 'Palestinians']]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_relation_triples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9df44f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4982931998556999, 0.4109760802469135)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(recall_avg) / len(recall_avg), sum(precision_avg) / len(precision_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "29c39271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1059, 297, 199)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "their,ours,common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe119000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
